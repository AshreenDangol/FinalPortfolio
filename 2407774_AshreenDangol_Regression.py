# -*- coding: utf-8 -*-
"""REGRESSIONfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118dXIlmOczpypmARQBdqd2JVoEdReZNt

# **IMPORT LIBRARY AND DATA LOADING**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/CTA/Datasets/STAR.csv')

# Display the first few rows of the dataset
print(data.head())

# Summary statistics
print(data.describe())

# Check for missing values
print(data.isnull().sum())

"""# **DATA VISUALIZATION**"""

# Correlation heatmap
plt.figure(figsize=(10, 8))
numerical_data = data.select_dtypes(include=np.number)
correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
plt.figure(figsize=(12, 6))

# Create a histogram with KDE
sns.histplot(data=data, x='Absolute magnitude(Mv)', bins=60, kde=True, color='blue', alpha=0.6)

# Set the title and labels
plt.title('Histogram of Absolute Magnitude (Mv) with KDE')
plt.xlabel('Absolute Magnitude (Mv)')
plt.ylabel('Count')

# Show the plot
plt.show()

# Set the figure size
plt.figure(figsize=(12, 6))

# Create a scatter plot
sns.scatterplot(data=data, x='Absolute magnitude(Mv)', y='Temperature (K)', color='blue',alpha=0.6,)

# Set the title and labels
plt.title('Scatter Plot of Absolute Magnitude vs. Temperature (K)')
plt.xlabel('Absolute Magnitude (Mv)')
plt.ylabel('Temperature (K)')

# Show the plot
plt.show()

# Set the figure size
plt.figure(figsize=(12, 6))

# Create a scatter plot
sns.scatterplot(data=data, x='Absolute magnitude(Mv)', y='Luminosity(L/Lo)', alpha=0.6)

# Set the title and labels
plt.title('Scatter Plot of Absolute Magnitude vs. Luminosity (L/Lo)')
plt.xlabel('Absolute Magnitude (Mv)')
plt.ylabel('Luminosity (L/Lo)')

# Show the plot
plt.show()

# Set the figure size
plt.figure(figsize=(12, 6))

# Create a scatter plot
sns.scatterplot(data=data, x='Absolute magnitude(Mv)', y='Radius(R/Ro)', alpha=0.6)

# Set the title and labels
plt.title('Scatter Plot of Absolute Magnitude vs. Radius(R/Ro)')
plt.xlabel('Absolute Magnitude (Mv)')
plt.ylabel('Radius(R/Ro)')

# Show the plot
plt.show()

"""# **LINEAR REGRESSION**"""



class SimpleLinearRegression:
    def __init__(self):
        self.b0 = 0
        self.b1 = 0

    def fit(self, X, y):
        n = len(X)
        mean_x, mean_y = np.mean(X), np.mean(y)

        # Calculate coefficients
        SS_xy = np.sum(y*X) - n*mean_y*mean_x
        SS_xx = np.sum(X*X) - n*mean_x*mean_x
        self.b1 = SS_xy / SS_xx
        self.b0 = mean_y - self.b1 * mean_x

    def predict(self, X):
        return self.b0 + self.b1 * X

# Example usage
X = data['Temperature (K)'].values.reshape(-1, 1)
y = data['Absolute magnitude(Mv)'].values

model_scratch = SimpleLinearRegression()
model_scratch.fit(X, y)
predictions_scratch = model_scratch.predict(X)

# Plotting the results
plt.scatter(X, y, color='blue', label='Actual')
plt.plot(X, predictions_scratch, color='red', label='Predicted')
plt.xlabel('Temperature (K)')
plt.ylabel('Absolute magnitude(Mv)')
plt.title('Simple Linear Regression from Scratch')
plt.legend()
plt.show()

"""### **MODEL1 **"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Split the data
X = data.drop('Absolute magnitude(Mv)', axis=1)
X = pd.get_dummies(X, columns=['Spectral Class', 'Star color'], drop_first=True)
y = data['Absolute magnitude(Mv)']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Predict and evaluate
y_pred_lr = model_lr.predict(X_test)
mse_lr = mean_squared_error(y_test, y_pred_lr)
print(f'Mean Squared Error (Linear Regression): {mse_lr}')

"""### **MODEL2**"""

from sklearn.ensemble import RandomForestRegressor

# Train the model
model_rf = RandomForestRegressor(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

# Predict and evaluate
y_pred_rf = model_rf.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'Mean Squared Error (Random Forest): {mse_rf}')

"""# Hyper-parameter Optimizations with Cross Validations"""



from sklearn.model_selection import GridSearchCV

# No hyperparameters to tune for Linear Regression, but we can still use GridSearchCV for completeness
param_grid_lr = {'fit_intercept': [True, False]}
grid_search_lr = GridSearchCV(LinearRegression(), param_grid_lr, cv=5)
grid_search_lr.fit(X_train, y_train)

best_params_lr = grid_search_lr.best_params_
print(f'Best Parameters (Linear Regression): {best_params_lr}')

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5)
grid_search_rf.fit(X_train, y_train)

best_params_rf = grid_search_rf.best_params_
print(f'Best Parameters (Random Forest): {best_params_rf}')

"""# **FEATURE SELECTION**"""

from sklearn.feature_selection import RFE

# Use the best parameters found in cross-validation
model_lr_best = LinearRegression()

# RFE
rfe_lr = RFE(model_lr_best, n_features_to_select=3)
rfe_lr.fit(X_train, y_train)

# Selected features
selected_features_lr = X.columns[rfe_lr.support_]
print(f'Selected Features (Linear Regression): {selected_features_lr}')

# Use the best parameters found in cross-validation
model_rf_best = RandomForestRegressor(**best_params_rf, random_state=42)

# RFE
rfe_rf = RFE(model_rf_best, n_features_to_select=3)
rfe_rf.fit(X_train, y_train)

# Selected features
selected_features_rf = X.columns[rfe_rf.support_]
print(f'Selected Features (Random Forest): {selected_features_rf}')

"""# **FINAL MODEL**"""

from sklearn.ensemble import RandomForestRegressor
# Using Random Forest as it performed better
final_model = RandomForestRegressor(**best_params_rf, random_state=42)
final_X_train = X_train[selected_features_rf]
final_X_test = X_test[selected_features_rf]
final_model.fit(final_X_train, y_train)
final_y_pred = final_model.predict(final_X_test)
final_mse = mean_squared_error(y_test, final_y_pred)
print(f'Final Mean Squared Error (Random Forest with Selected Features): {final_mse}')
previous_mse = 1.1661324966541653

# Compare MSE
if final_mse < previous_mse:
    print("Model performance improved.")
else:
    print("Model performance did not improve.")